{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69906a08",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq langgraph uv databricks-agents mlflow-skinny[databricks] databricks-mcp databricks-langchain\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc1454",
   "metadata": {},
   "source": [
    "# Below code will generate a agent.py in your directory in databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dc9f19",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "import asyncio\n",
    "from typing import Annotated, Any, Generator, List, Optional, Sequence, TypedDict, Union\n",
    "\n",
    "import mlflow\n",
    "import nest_asyncio\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    VectorSearchRetrieverTool,\n",
    ")\n",
    "from databricks_mcp import DatabricksMCPClient, DatabricksOAuthClientProvider\n",
    "from langchain.messages import AIMessage, AIMessageChunk, AnyMessage\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client as connect\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "from pydantic import create_model\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "############################################\n",
    "## Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "# TODO: Replace with your model serving endpoint\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-sonnet-4-5\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "# TODO: Update with your system prompt\n",
    "system_prompt = \"\"\"You are an assitant\"\"\"\n",
    "\n",
    "###############################################################################\n",
    "## Configure MCP Servers for your agent\n",
    "##\n",
    "## This section sets up server connections so your agent can retrieve data or take actions.\n",
    "\n",
    "## There are three connection types:\n",
    "## 1. Managed MCP servers — fully managed by Databricks (no setup required)\n",
    "## 2. External MCP servers — hosted outside Databricks but proxied through a\n",
    "##    Managed MCP server proxy (some setup required)\n",
    "## 3. Custom MCP servers — MCP servers hosted as Databricks Apps (OAuth setup required)\n",
    "##\n",
    "## Note: External MCP servers get added to the \"managed\" URL list\n",
    "## because their proxy endpoints are managed by Databricks.\n",
    "###############################################################################\n",
    "\n",
    "# TODO: Choose your MCP server connection type and fill in the appropriate URLs.\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Managed MCP Server — simplest setup\n",
    "# ---------------------------------------------------------------------------\n",
    "# Databricks manages this connection automatically using your workspace settings\n",
    "# and Personal Access Token (PAT) authentication.\n",
    "\n",
    "workspace_client = WorkspaceClient()\n",
    "\n",
    "host = workspace_client.config.host\n",
    "\n",
    "# Managed MCP Servers URLS (includes both fully managed and proxied external MCP)\n",
    "# - If you're using an external MCP server, create a UC connection and flag it\n",
    "#   as an MCP connection. This reveals a proxy endpoint.\n",
    "# - Add that proxy endpoint URL to this list.\n",
    "\n",
    "MANAGED_MCP_SERVER_URLS = [\n",
    "    f\"{host}/api/2.0/mcp/vector-search/agentic_systems/customer_support\",\n",
    "    f\"{host}/api/2.0/mcp/genie/<<ENTER_GENIE_SPACE_ID_HERE>>\",\n",
    "    f\"{host}/api/2.0/mcp/sql\",\n",
    "    f\"{host}/api/2.0/mcp/functions/<<ENTER_UC_FUNCTION_PATH_HERE>>\"\n",
    "]\n",
    "\n",
    "#    f\"{host}/api/2.0/mcp/functions/genie_dev_catalog/default\",\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Custom MCP Server — hosted as a Databricks App\n",
    "# ---------------------------------------------------------------------------\n",
    "# Use this if you’re running your own MCP server in Databricks.\n",
    "# These require OAuth with a service principal for machine-to-machine (M2M) auth.\n",
    "#\n",
    "# Uncomment and fill in the settings below to use a custom MCP server.\n",
    "#\n",
    "# import os\n",
    "# workspace_client = WorkspaceClient(\n",
    "#     host=\"<<ENTER_DATABRICKS_WORKSPACE_URL>>\",\n",
    "#     client_id=\"<<ENTER_DATABRICKS_APPS_CLIENT_ID>>\",\n",
    "#     client_secret=\"<<ENTER_DBX_PAT_TOKEN>>\",\n",
    "#     auth_type=\"oauth-m2m\",  # Enables service principal authentication\n",
    "# )\n",
    "\n",
    "# # Custom MCP Servers — add URLs below (not managed or proxied by Databricks)\n",
    "CUSTOM_MCP_SERVER_URLS = [\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "#####################\n",
    "## MCP Tool Creation\n",
    "#####################\n",
    "\n",
    "# Define a custom LangChain tool that wraps functionality for calling MCP servers\n",
    "class MCPTool(BaseTool):\n",
    "    \"\"\"Custom LangChain tool that wraps MCP server functionality\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        description: str,\n",
    "        args_schema: type,\n",
    "        server_url: str,\n",
    "        ws: WorkspaceClient,\n",
    "        is_custom: bool = False,\n",
    "    ):\n",
    "        # Initialize the tool\n",
    "        super().__init__(name=name, description=description, args_schema=args_schema)\n",
    "        # Store custom attributes: MCP server URL, Databricks workspace client, and whether the tool is for a custom server\n",
    "        object.__setattr__(self, \"server_url\", server_url)\n",
    "        object.__setattr__(self, \"workspace_client\", ws)\n",
    "        object.__setattr__(self, \"is_custom\", is_custom)\n",
    "\n",
    "    def _run(self, **kwargs) -> str:\n",
    "        \"\"\"Execute the MCP tool\"\"\"\n",
    "        if self.is_custom:\n",
    "            # Use the async method for custom MCP servers (OAuth required)\n",
    "            return asyncio.run(self._run_custom_async(**kwargs))\n",
    "        else:\n",
    "            # Use managed MCP server via synchronous call\n",
    "            mcp_client = DatabricksMCPClient(\n",
    "                server_url=self.server_url, workspace_client=self.workspace_client\n",
    "            )\n",
    "            response = mcp_client.call_tool(self.name, kwargs)\n",
    "            return \"\".join([c.text for c in response.content])\n",
    "\n",
    "    async def _run_custom_async(self, **kwargs) -> str:\n",
    "        \"\"\"Execute custom MCP tool asynchronously\"\"\"\n",
    "        async with connect(\n",
    "            self.server_url, auth=DatabricksOAuthClientProvider(self.workspace_client)\n",
    "        ) as (\n",
    "            read_stream,\n",
    "            write_stream,\n",
    "            _,\n",
    "        ):\n",
    "            # Create an async session with the server and call the tool\n",
    "            async with ClientSession(read_stream, write_stream) as session:\n",
    "                await session.initialize()\n",
    "                response = await session.call_tool(self.name, kwargs)\n",
    "                return \"\".join([c.text for c in response.content])\n",
    "\n",
    "\n",
    "# Retrieve tool definitions from a custom MCP server (OAuth required)\n",
    "async def get_custom_mcp_tools(ws: WorkspaceClient, server_url: str):\n",
    "    \"\"\"Get tools from a custom MCP server using OAuth\"\"\"\n",
    "    async with connect(server_url, auth=DatabricksOAuthClientProvider(ws)) as (\n",
    "        read_stream,\n",
    "        write_stream,\n",
    "        _,\n",
    "    ):\n",
    "        async with ClientSession(read_stream, write_stream) as session:\n",
    "            await session.initialize()\n",
    "            tools_response = await session.list_tools()\n",
    "            return tools_response.tools\n",
    "\n",
    "\n",
    "# Retrieve tool definitions from a managed MCP server\n",
    "def get_managed_mcp_tools(ws: WorkspaceClient, server_url: str):\n",
    "    \"\"\"Get tools from a managed MCP server\"\"\"\n",
    "    mcp_client = DatabricksMCPClient(server_url=server_url, workspace_client=ws)\n",
    "    return mcp_client.list_tools()\n",
    "\n",
    "\n",
    "# Convert an MCP tool definition into a LangChain-compatible tool\n",
    "def create_langchain_tool_from_mcp(\n",
    "    mcp_tool, server_url: str, ws: WorkspaceClient, is_custom: bool = False\n",
    "):\n",
    "    \"\"\"Create a LangChain tool from an MCP tool definition\"\"\"\n",
    "    schema = mcp_tool.inputSchema.copy()\n",
    "    properties = schema.get(\"properties\", {})\n",
    "    required = schema.get(\"required\", [])\n",
    "\n",
    "    # Map JSON schema types to Python types for input validation\n",
    "    TYPE_MAPPING = {\"integer\": int, \"number\": float, \"boolean\": bool}\n",
    "    field_definitions = {}\n",
    "    for field_name, field_info in properties.items():\n",
    "        field_type_str = field_info.get(\"type\", \"string\")\n",
    "        field_type = TYPE_MAPPING.get(field_type_str, str)\n",
    "\n",
    "        if field_name in required:\n",
    "            field_definitions[field_name] = (field_type, ...)\n",
    "        else:\n",
    "            field_definitions[field_name] = (field_type, None)\n",
    "\n",
    "    # Dynamically create a Pydantic schema for the tool's input arguments\n",
    "    args_schema = create_model(f\"{mcp_tool.name}Args\", **field_definitions)\n",
    "\n",
    "    # Return a configured MCPTool instance\n",
    "    return MCPTool(\n",
    "        name=mcp_tool.name,\n",
    "        description=mcp_tool.description or f\"Tool: {mcp_tool.name}\",\n",
    "        args_schema=args_schema,\n",
    "        server_url=server_url,\n",
    "        ws=ws,\n",
    "        is_custom=is_custom,\n",
    "    )\n",
    "\n",
    "\n",
    "# Gather all tools from managed and custom MCP servers into a single list\n",
    "async def create_mcp_tools(\n",
    "    ws: WorkspaceClient, managed_server_urls: List[str] = None, custom_server_urls: List[str] = None\n",
    ") -> List[MCPTool]:\n",
    "    \"\"\"Create LangChain tools from both managed and custom MCP servers\"\"\"\n",
    "    tools = []\n",
    "\n",
    "    if managed_server_urls:\n",
    "        # Load managed MCP tools\n",
    "        for server_url in managed_server_urls:\n",
    "            try:\n",
    "                mcp_tools = get_managed_mcp_tools(ws, server_url)\n",
    "                for mcp_tool in mcp_tools:\n",
    "                    tool = create_langchain_tool_from_mcp(mcp_tool, server_url, ws, is_custom=False)\n",
    "                    tools.append(tool)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading tools from managed server {server_url}: {e}\")\n",
    "\n",
    "    if custom_server_urls:\n",
    "        # Load custom MCP tools (async)\n",
    "        for server_url in custom_server_urls:\n",
    "            try:\n",
    "                mcp_tools = await get_custom_mcp_tools(ws, server_url)\n",
    "                for mcp_tool in mcp_tools:\n",
    "                    tool = create_langchain_tool_from_mcp(mcp_tool, server_url, ws, is_custom=True)\n",
    "                    tools.append(tool)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading tools from custom server {server_url}: {e}\")\n",
    "\n",
    "    return tools\n",
    "\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "\n",
    "# The state for the agent workflow, including the conversation and any custom data\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], add_messages]\n",
    "    custom_inputs: Optional[dict[str, Any]]\n",
    "    custom_outputs: Optional[dict[str, Any]]\n",
    "\n",
    "\n",
    "# Define the LangGraph agent that can call tools\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    "):\n",
    "    model = model.bind_tools(tools)  # Bind tools to the model\n",
    "\n",
    "    # Function to check if agent should continue or finish based on last message\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If function (tool) calls are present, continue; otherwise, end\n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    # Preprocess: optionally prepend a system prompt to the conversation history\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "\n",
    "    model_runnable = preprocessor | model  # Chain the preprocessor and the model\n",
    "\n",
    "    # The function to invoke the model within the workflow\n",
    "    def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)  # Create the agent's state machine\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))  # Agent node (LLM)\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))  # Tools node\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")  # Start at agent node\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",  # If the model requests a tool call, move to tools node\n",
    "            \"end\": END,  # Otherwise, end the workflow\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")  # After tools are called, return to agent node\n",
    "\n",
    "    # Compile and return the tool-calling agent workflow\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "# ResponsesAgent class to wrap the compiled agent and make it compatible with Mosaic AI Responses API\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    # Make a prediction (single-step) for the agent\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\" or event.type == \"error\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    # Stream predictions for the agent, yielding output as it's generated\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        cc_msgs = to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        # Stream events from the agent graph\n",
    "        for event in self.agent.stream({\"messages\": cc_msgs}, stream_mode=[\"updates\", \"messages\"]):\n",
    "            if event[0] == \"updates\":\n",
    "                # Stream updated messages from the workflow nodes\n",
    "                for node_data in event[1].values():\n",
    "                    if len(node_data.get(\"messages\", [])) > 0:\n",
    "                        yield from output_to_responses_items_stream(node_data[\"messages\"])\n",
    "            elif event[0] == \"messages\":\n",
    "                # Stream generated text message chunks\n",
    "                try:\n",
    "                    chunk = event[1][0]\n",
    "                    if isinstance(chunk, AIMessageChunk) and (content := chunk.content):\n",
    "                        yield ResponsesAgentStreamEvent(\n",
    "                            **self.create_text_delta(delta=content, item_id=chunk.id),\n",
    "                        )\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "# Initialize the entire agent, including MCP tools and workflow\n",
    "def initialize_agent():\n",
    "    \"\"\"Initialize the agent with MCP tools\"\"\"\n",
    "    # Create MCP tools from the configured servers\n",
    "    mcp_tools = asyncio.run(\n",
    "        create_mcp_tools(\n",
    "            ws=workspace_client,\n",
    "            managed_server_urls=MANAGED_MCP_SERVER_URLS,\n",
    "            custom_server_urls=CUSTOM_MCP_SERVER_URLS,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the agent graph with an LLM, tool set, and system prompt (if given)\n",
    "    agent = create_tool_calling_agent(llm, mcp_tools, system_prompt)\n",
    "    return LangGraphResponsesAgent(agent)\n",
    "\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = initialize_agent()\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9798dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7fbb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "AGENT.predict({\"input\": [{\"role\": \"user\", \"content\": \"what tools you have access to\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in AGENT.predict_stream(\n",
    "    {\"input\": [{\"role\": \"user\", \"content\": \"generate top 5 countries with higher health expectancy rate\"}]}\n",
    "):\n",
    "    print(chunk, \"-----------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46abdf3",
   "metadata": {},
   "source": [
    "#Adding MLFLOW to log the resources and model to UC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from agent import LLM_ENDPOINT_NAME\n",
    "from mlflow.models.resources import DatabricksServingEndpoint, DatabricksFunction, DatabricksGenieSpace, DatabricksVectorSearchIndex, DatabricksSQLWarehouse, DatabricksTable\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME),\n",
    "    DatabricksGenieSpace(genie_space_id=\"<<ENTER_YOUR_GENIE_SPACE_ID_HERE>>\"),\n",
    "    DatabricksSQLWarehouse(warehouse_id=\"<<ENTER_YOUR_SQL_WAREHOUSE_ID_HERE>>\"),\n",
    "    DatabricksTable(table_name=\"<<ENTER_GENIE_CATALOG_TABLE_NAME_HERE>>\"),\n",
    "    DatabricksTable(table_name=\"<<ENTER_UC_FUNCTION_CATALOG_TABLE_NAME_HERE>>\"),\n",
    "    DatabricksVectorSearchIndex(index_name=\"<<ENTER_YOUR_VECTOR_SEARCH_INDEX_NAME_HERE>>\"),\n",
    "    DatabricksFunction(function_name=\"<<ENTER_YOUR_UC_FUNCTION_NAME_HERE>>\"),\n",
    "]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            \"databricks-mcp\",\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "            f\"mcp=={get_distribution('mcp').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065dda01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e4c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import RelevanceToQuery, Safety, RetrievalRelevance, RetrievalGroundedness\n",
    "\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"which country has highest health expectancy\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"expected_response\": \"\"\"Singapore leads with the highest healthy life expectancy at 73.62 years\n",
    "        Japan follows closely with 73.16 years\n",
    "        All top 5 countries have healthy life expectancy rates above 71 years\n",
    "        The data is from 2016, representing the most recent year available in the dataset\n",
    "        These countries are known for their excellent healthcare systems, healthy lifestyles, and high quality of life\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=lambda input: AGENT.predict({\"input\": input}),\n",
    "    scorers=[RelevanceToQuery(), Safety()], # add more scorers here if they're applicable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f85dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"which country has highest health expectancy\"}]},\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1261be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = \"<<ENTER_YOUR_UC_CATALOG_NAME_HERE>>\"\n",
    "schema = \"<<ENTER_YOUR_UC_SCHEMA_NAME_HERE>>\"\n",
    "model_name = \"<<ENTER_YOUR_UC_MODEL_NAME_HERE>>\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc6984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "agents.deploy(\n",
    "    UC_MODEL_NAME, \n",
    "    uc_registered_model_info.version,\n",
    "    # ==============================================================================\n",
    "    # TODO: ONLY UNCOMMENT AND CONFIGURE THE ENVIRONMENT_VARS SECTION BELOW\n",
    "    #       IF YOU ARE USING OAUTH/SERVICE PRINCIPAL FOR CUSTOM MCP SERVERS.\n",
    "    #       For managed MCP (the default), LEAVE THIS SECTION COMMENTED OUT.\n",
    "    # ==============================================================================\n",
    "    # environment_vars={\n",
    "    #     \"DATABRICKS_CLIENT_ID\": DATABRICKS_CLIENT_ID,\n",
    "    #     \"DATABRICKS_CLIENT_SECRET\": f\"{{{{secrets/{client_secret_scope_name}/{client_secret_key_name}}}}}\"\n",
    "    # },\n",
    "    tags = {\"type\": \"pilot test\"},\n",
    "    deploy_feedback_model=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
